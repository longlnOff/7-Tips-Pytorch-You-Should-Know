{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Tensor Directly on the Target Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m      3\u001b[0m \t\u001b[39m# Create on the CPU, then transfering to the GPU\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \tcpu_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((\u001b[39m1000\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \tgpu_tensor \u001b[39m=\u001b[39m cpu_tensor\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m      6\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTotal time: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(end_time \u001b[39m-\u001b[39m start_time))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "\t# Create on the CPU, then transfering to the GPU\n",
    "\tcpu_tensor = torch.ones((1000, 64, 64))\n",
    "\tgpu_tensor = cpu_tensor.cuda()\n",
    "end_time = time.time()\n",
    "print('Total time: {:.3f}s'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m      3\u001b[0m \t\u001b[39m# Create directly on the GPU\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \tgpu_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mones((\u001b[39m1000\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m64\u001b[39;49m), device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTotal time: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(end_time \u001b[39m-\u001b[39m start_time))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "\t# Create directly on the GPU\n",
    "\tgpu_tensor = torch.ones((1000, 64, 64), device='cuda')\n",
    "end_time = time.time()\n",
    "print('Total time: {:.3f}s'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Sequential Layers When Possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModelDont(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        input_size      = 2\n",
    "        output_size     = 3\n",
    "        hidden_size     = 16\n",
    "\n",
    "        self.inputlayer         = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.inputactivation    = torch.nn.ReLU()\n",
    "\n",
    "        self.midlayer           = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.midactivation      = torch.nn.ReLU()\n",
    "\n",
    "        self.outputlayer        = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.outputactivation   = torch.nn.Softmax()\n",
    "\n",
    "    def foward(self, input):\n",
    "        input           = self.inputlayer(input)\n",
    "        input           = self.inputactivation(input)\n",
    "\n",
    "        input           = self.midlayer(input)\n",
    "        input           = self.midactivation(input)\n",
    "\n",
    "        input           = self.outputlayer(input)\n",
    "        output          = self.outputactivation(input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleSequentialModel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        input_size      = 2\n",
    "        output_size     = 3\n",
    "        hidden_size     = 16\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def foward(self, input):\n",
    "        output = self.layers(input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Don't Make Lists of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BadListExample(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        input_size      = 2\n",
    "        output_size     = 3\n",
    "        hidden_size     = 16\n",
    "\n",
    "        self.inputlayer         = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.inputactivation    = torch.nn.ReLU()\n",
    "\n",
    "        # Common error when using list layers\n",
    "        self.midlayers          = []\n",
    "        for _ in range(5):\n",
    "            self.midlayers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            self.midlayers.append(torch.nn.ReLU())\n",
    "        \n",
    "        self.outputlayer        = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.outputactivation   = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.inputlayer(input)\n",
    "        input = self.inputactivation(input)\n",
    "\n",
    "        for layer in self.midlayers:\n",
    "            input = layer(input)\n",
    "\n",
    "        input = self.outputlayer(input)\n",
    "        out   = self.outputactivation(input)\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Vấn đề khi sử dụng list model là khi ta đưa model và data vào GPU, list sẽ không được registry trong GPU -> xung đột device giữa model và data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BadListExample(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        input_size      = 2\n",
    "        output_size     = 3\n",
    "        hidden_size     = 16\n",
    "\n",
    "        self.inputlayer         = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.inputactivation    = torch.nn.ReLU()\n",
    "\n",
    "        # Common error when using list layers\n",
    "        self.midlayers          = []\n",
    "        for _ in range(5):\n",
    "            self.midlayers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            self.midlayers.append(torch.nn.ReLU())\n",
    "\n",
    "        # Fix list layers\n",
    "        self.midlayers          = torch.nn.Sequential(*self.midlayers)\n",
    "\n",
    "        self.outputlayer        = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.outputactivation   = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.inputlayer(input)\n",
    "        input = self.inputactivation(input)\n",
    "        input = self.midlayers(input)\n",
    "        input = self.outputlayer(input)\n",
    "        out   = self.outputactivation(input)\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make Use of Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use detach() On Long-Term Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m data_batches:\n\u001b[1;32m      3\u001b[0m     output \u001b[39m=\u001b[39m example_model(batch)\n\u001b[1;32m      5\u001b[0m     target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand((\u001b[39m10\u001b[39m,\u001b[39m3\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_batches' is not defined"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for batch in data_batches:\n",
    "    output = example_model(batch)\n",
    "\n",
    "    target = torch.rand((10,3))\n",
    "    loss = metric(output, target)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for batch in data_batches:\n",
    "    output = example_model(batch)\n",
    "\n",
    "    target = torch.rand((10,3))\n",
    "    loss = metric(output, target)\n",
    "    # Fix don't to do\n",
    "    losses.append(loss.detach()) # or can use loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trick to Delete a Model from GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "model = Model().cuda()\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "# The model will normally stay on the \n",
    "# cache until something takes it's place\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Call eval() Before Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "# Training\n",
    "model.train()\n",
    "# End training\n",
    "# Evaluating\n",
    "model.eval()\n",
    "# End evaluating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
